FROM eclipse-temurin:11-jre

# Definir versões
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV JAVA_HOME=/opt/java/openjdk

# Instalar dependências necessárias
RUN apt-get update && \
    apt-get install -y \
    curl \
    procps \
    python3 \
    python3-pip \
    netcat-traditional \
    && rm -rf /var/lib/apt/lists/*

# Baixar e instalar Apache Spark
RUN curl -fsSL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    rm -rf ${SPARK_HOME}/examples ${SPARK_HOME}/data

# Configurar variáveis de ambiente
ENV PATH="${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"
ENV PYTHONPATH="${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:${PYTHONPATH}"
ENV PYSPARK_PYTHON=python3

# Instalar bibliotecas Python essenciais para Spark
RUN pip3 install --no-cache-dir --break-system-packages \
    pyspark==${SPARK_VERSION} \
    pandas \
    numpy \
    pyarrow

# Criar diretórios necessários
RUN mkdir -p \
    /opt/spark-apps \
    /opt/spark-data \
    /opt/spark-logs \
    ${SPARK_HOME}/work \
    ${SPARK_HOME}/logs

# Configurar permissões
RUN chmod -R 777 /opt/spark-apps /opt/spark-data /opt/spark-logs ${SPARK_HOME}/work ${SPARK_HOME}/logs

# Configuração de log do Spark
RUN echo "log4j.rootCategory=WARN, console" > ${SPARK_HOME}/conf/log4j.properties && \
    echo "log4j.appender.console=org.apache.log4j.ConsoleAppender" >> ${SPARK_HOME}/conf/log4j.properties && \
    echo "log4j.appender.console.target=System.err" >> ${SPARK_HOME}/conf/log4j.properties && \
    echo "log4j.appender.console.layout=org.apache.log4j.PatternLayout" >> ${SPARK_HOME}/conf/log4j.properties && \
    echo "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n" >> ${SPARK_HOME}/conf/log4j.properties

WORKDIR ${SPARK_HOME}

# Expor portas
# 7077: Spark Master
# 8080: Spark Master Web UI
# 8081: Spark Worker Web UI
# 4040: Spark Application UI
EXPOSE 7077 8080 8081 4040

# Comando padrão (pode ser sobrescrito no docker-compose)
CMD ["bin/spark-class", "org.apache.spark.deploy.master.Master"]
